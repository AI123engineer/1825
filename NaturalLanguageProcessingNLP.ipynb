{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "NaturalLanguageProcessingNLP.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjfZu8qiTLck",
        "outputId": "9ae16ee8-f492-4df9-d0a3-e2d4ccb2e1f9"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.5)\n",
            "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2020.6.8)\n",
            "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (0.16.0)\n",
            "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.47.0)\n",
            "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dcfcstQTLcs"
      },
      "source": [
        "# Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLtFnd0qTLcu"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZpYejCxTLcv",
        "outputId": "0a47b38e-ed9a-4eb6-fc97-893ef676fca9"
      },
      "source": [
        "#After importing nltk, download this\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\Jerish\n",
            "[nltk_data]     B\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxSP9jMBTLcw"
      },
      "source": [
        "#Define your text or import from other source\n",
        "text='I am learning Natural Language Processing'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9egacsELTLcy",
        "outputId": "30329d84-8760-493c-990b-0c93286349b8"
      },
      "source": [
        "#Tokenizing the text\n",
        "print(word_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'am', 'learning', 'Natural', 'Language', 'Processing']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D6RPOZ_TLcz"
      },
      "source": [
        "# Sentence Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc1gEVeVTLcz"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-ZDynoLTLc0"
      },
      "source": [
        "text=\"Good Morning! How are you?\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpETiEXMTLc1",
        "outputId": "9228c94b-b092-4636-9a3d-8a84cf4771d4"
      },
      "source": [
        "#Tokenizing the text\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Good Morning!', 'How are you?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzLCVnbLTLc3",
        "outputId": "0f90c50d-cdf7-470f-d9cf-4fd682c4fbd5"
      },
      "source": [
        "text=\"Our company annual growth rate is 25.50%. Good job Mr.Bajaj\"\n",
        "#Tokenizing the text\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Our company annual growth rate is 25.50%.', 'Good job Mr.Bajaj']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9qtQIndTLc4",
        "outputId": "3dc1509e-b48f-400b-d9fc-3949dce8916a"
      },
      "source": [
        "text=\"Good. Morning! How are you?.\"\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Good.', 'Morning!', 'How are you?.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRCvmEClTLc5"
      },
      "source": [
        "# Regular Expressions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNHvBmn9TLc6"
      },
      "source": [
        "from nltk.tokenize import regexp_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjTrlW35TLc9"
      },
      "source": [
        "text=\"NLP is fun and Can deal with texts and sounds, but can't deal with images. We have a session at 11AM!.We can earn lot of $\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIP2G5lVTLc9",
        "outputId": "8005b1c9-e316-408e-ebff-1884bda51331"
      },
      "source": [
        "#Printing only word by word that contains all small case letters starting from a till z\n",
        "regexp_tokenize(text,\"[a-z]+\")\n",
        "\n",
        "#Even apostrophe words are separated because of this"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['is',\n",
              " 'fun',\n",
              " 'and',\n",
              " 'an',\n",
              " 'deal',\n",
              " 'with',\n",
              " 'texts',\n",
              " 'and',\n",
              " 'sounds',\n",
              " 'but',\n",
              " 'can',\n",
              " 't',\n",
              " 'deal',\n",
              " 'with',\n",
              " 'images',\n",
              " 'e',\n",
              " 'have',\n",
              " 'a',\n",
              " 'session',\n",
              " 'at',\n",
              " 'e',\n",
              " 'can',\n",
              " 'earn',\n",
              " 'lot',\n",
              " 'of']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOxsXFF_TLc-",
        "outputId": "7b653384-c596-42a1-8b5a-bebd159a92b0"
      },
      "source": [
        "#For considering apostrophe words as single word, use the below code\n",
        "regexp_tokenize(text,\"[a-z']+\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['is',\n",
              " 'fun',\n",
              " 'and',\n",
              " 'an',\n",
              " 'deal',\n",
              " 'with',\n",
              " 'texts',\n",
              " 'and',\n",
              " 'sounds',\n",
              " 'but',\n",
              " \"can't\",\n",
              " 'deal',\n",
              " 'with',\n",
              " 'images',\n",
              " 'e',\n",
              " 'have',\n",
              " 'a',\n",
              " 'session',\n",
              " 'at',\n",
              " 'e',\n",
              " 'can',\n",
              " 'earn',\n",
              " 'lot',\n",
              " 'of']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRsu7GRxTLc_",
        "outputId": "ff88537c-d4b9-4dbf-c03d-75f3fcda3d74"
      },
      "source": [
        "#Printing only word by word that contains all capital letters starting from A till Z\n",
        "regexp_tokenize(text,\"[A-Z]+\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP', 'C', 'W', 'AM', 'W']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBBJOt8xTLdA",
        "outputId": "0103b1c9-af22-40d6-8cca-84cf25dc2a00"
      },
      "source": [
        "#Printing all words with apostrophe in a single line\n",
        "regexp_tokenize(text,\"[\\a-z']+\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"NLP is fun and Can deal with texts and sounds, but can't deal with images. We have a session at 11AM!.We can earn lot of $\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1YLEBsTTLdB",
        "outputId": "d0609397-ede1-4f8b-a710-0f17bf2796d9"
      },
      "source": [
        "#Anything starts with caret is not equal\n",
        "regexp_tokenize(text,\"[^a-z']+\") \n",
        "\n",
        "#Here it returns spaces for small letters and apostrophe and prints all capital words and symbols"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' C',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ', ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " '. W',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' 11AM!.W',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' $']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GANlzX0ZTLdC",
        "outputId": "aaef0bbb-b9b6-4148-8b9e-c6e9abe28fb4"
      },
      "source": [
        "#Printing only numbers\n",
        "regexp_tokenize(text,\"[0-9]+\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['11']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6qA1qX5TLdD",
        "outputId": "ccc747e9-cde4-4a45-81f0-a6022bfc3be4"
      },
      "source": [
        "#Printing all without numbers\n",
        "regexp_tokenize(text,\"[^0-9]+\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"NLP is fun and Can deal with texts and sounds, but can't deal with images. We have a session at \",\n",
              " 'AM!.We can earn lot of $']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6nYhOYdTLdD",
        "outputId": "925a2713-c261-4aaf-c188-869cf9c8ac09"
      },
      "source": [
        "#Printing only special expression\n",
        "regexp_tokenize(text,\"[$]\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['$']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5Z28tioTLdE"
      },
      "source": [
        "# Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68MoCTfbTLdF"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "#If we get error, use nltk.download('stopwords')\n",
        "#Corpus is a large and structured set of texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AcLBj6ETLdF",
        "outputId": "e2ff8cc0-b682-466c-83a8-6a9791f9d328"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\Jerish\n",
            "[nltk_data]     B\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ylKvSfyTLdG"
      },
      "source": [
        "#All stopwords in english language\n",
        "stop_words=stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMW3ALdMTLdG",
        "outputId": "d858b755-b1f8-4adf-8f19-cc96924bf4c6"
      },
      "source": [
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5SyV7xGTLdH",
        "outputId": "5a240341-b8b8-4d4a-a9b2-59c6c0688e13"
      },
      "source": [
        "len(stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41cl7GTyTLdH"
      },
      "source": [
        "#Another way to import stopwords\n",
        "import nltk\n",
        "stopset=set(nltk.corpus.stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDfbWO0gTLdI",
        "outputId": "aed3e5f4-13fb-40ac-ff8c-62b434b91232"
      },
      "source": [
        "len(stopset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSd8kE2wTLdJ",
        "outputId": "f82c4eb1-3a67-4e0d-c1a9-b08bfca48f3e"
      },
      "source": [
        "#We can customize stopwords by using update\n",
        "stopset.update(('new','old'))\n",
        "len(stopset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "181"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3aakctuTLdK",
        "outputId": "9b4981b0-cc62-4530-9c67-a10896462d2f"
      },
      "source": [
        "#Checking stopwords after customization\n",
        "stopset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'new',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'old',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmZL7bGYTLdL"
      },
      "source": [
        "# Handling punctuations using string module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDF_La8UTLdL"
      },
      "source": [
        "import string"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va6R_rYtTLdN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fac6b2cc-8bae-4750-b550-7d9c77c90c88"
      },
      "source": [
        "#Checking all punctuations using string module\n",
        "string.punctuation"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jSmjQ8LTLdO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "aa0419af-dba7-4943-9d34-c4e3f11348bb"
      },
      "source": [
        "#We will try to remove stopwords and punctuations from a given text\n",
        "stop_words=stopwords.words('english')\n",
        "punct=string.punctuation"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1f4ab95e3209>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#We will try to remove stopwords and punctuations from a given text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpunct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf2-5XYmTLdP"
      },
      "source": [
        "text=\"India (Hindi: Bhārat), officially the Republic of India (Hindi: Bhārat Gaṇarājya),[23] is a country in South Asia. It is the second-most populous country, the seventh-largest country by land area, and the most populous democracy in the world. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west;[f] China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia.\"\n",
        "#Our text\n",
        "\n",
        "#Empty list to load cleaned data\n",
        "cleaned_text=[]\n",
        "\n",
        "#Taking for loop for iterating the text\n",
        "for word in nltk.word_tokenize(text):\n",
        "    if word not in punct:\n",
        "        if word not in stop_words:\n",
        "            cleaned_text.append(word)  #Appending all text excluding stop words and punctuations\n",
        "            \n",
        "print(\"Original length: \",len(text))\n",
        "print(\"Length of cleaned text: \",len(cleaned_text))\n",
        "print(\"\\n\",cleaned_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouxkPOfMTLdR"
      },
      "source": [
        "#Lower and upper case\n",
        "print(text.lower())\n",
        "print('\\n',text.upper())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXnDRf8hTLdS"
      },
      "source": [
        "# Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmqfDuq4TLdS"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_PIu3jATLdT"
      },
      "source": [
        "#Instantiation\n",
        "lancaster=LancasterStemmer()\n",
        "porter=PorterStemmer()\n",
        "snowball=SnowballStemmer('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LGu8aSzTLdT",
        "outputId": "95aa7ffb-5a93-4e88-b510-eaca3c00643f"
      },
      "source": [
        "#PorterStemmer\n",
        "print('Porter stemmer')\n",
        "print(porter.stem('hobby'))\n",
        "print(porter.stem('hobbies'))\n",
        "print(porter.stem('computer'))\n",
        "print(porter.stem('computation'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Porter stemmer\n",
            "hobbi\n",
            "hobbi\n",
            "comput\n",
            "comput\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_BsiAIvTLdU",
        "outputId": "3837db6b-999f-401d-b9e5-d76a7d38245d"
      },
      "source": [
        "#LancasterStemmer\n",
        "print('Lancaster stemmer')\n",
        "print(lancaster.stem('hobby'))\n",
        "print(lancaster.stem('hobbies'))\n",
        "print(lancaster.stem('computer'))\n",
        "print(lancaster.stem('computation'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lancaster stemmer\n",
            "hobby\n",
            "hobby\n",
            "comput\n",
            "comput\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "303b7ZRYTLdV",
        "outputId": "1487eadf-1e0a-4851-9ae5-2d55ebf751ed"
      },
      "source": [
        "#Tokenizing a sentence\n",
        "sentence=\"I was going to the office on my bike when i saw a car passing by hit the tree\"\n",
        "token=list(nltk.word_tokenize(sentence))\n",
        "token"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'was',\n",
              " 'going',\n",
              " 'to',\n",
              " 'the',\n",
              " 'office',\n",
              " 'on',\n",
              " 'my',\n",
              " 'bike',\n",
              " 'when',\n",
              " 'i',\n",
              " 'saw',\n",
              " 'a',\n",
              " 'car',\n",
              " 'passing',\n",
              " 'by',\n",
              " 'hit',\n",
              " 'the',\n",
              " 'tree']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt6Kc29vTLdW",
        "outputId": "95607412-2fae-4e68-eebb-5783216ed695"
      },
      "source": [
        "print(porter.stem('running'))\n",
        "print(porter.stem('runs'))\n",
        "print(porter.stem('ran'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run\n",
            "run\n",
            "ran\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6pH39wNTLdW",
        "outputId": "247bd82e-dd7c-4557-e3fb-e50a4bb347c6"
      },
      "source": [
        "sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I was going to the office on my bike when i saw a car passing by hit the tree'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MkyicGi2TLdX",
        "outputId": "1dc98dc2-620d-4fbf-c7c7-c4e5b77cb01e"
      },
      "source": [
        "for stemmer in (snowball, lancaster, porter):\n",
        "    stemm=[stemmer.stem(t) for t in token]\n",
        "    print(\" \".join(stemm))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i was go to the offic on my bike when i saw a car pass by hit the tree\n",
            "i was going to the off on my bik when i saw a car pass by hit the tre\n",
            "I wa go to the offic on my bike when i saw a car pass by hit the tree\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zfXnbABTLdY"
      },
      "source": [
        "# Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MAHIfV7TLdY"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CIpW0rATLdZ",
        "outputId": "1441d2a3-4222-45af-e647-b33d50413f48"
      },
      "source": [
        "#Download after importing WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to C:\\Users\\Jerish\n",
            "[nltk_data]     B\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hCg0FnaTLda"
      },
      "source": [
        "lemma=WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIUJm0XuTLda",
        "outputId": "5c3500a5-0967-4af5-800a-329f2efe5577"
      },
      "source": [
        "print(lemma.lemmatize('running'))\n",
        "print(lemma.lemmatize('runs'))\n",
        "print(lemma.lemmatize('ran'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running\n",
            "run\n",
            "ran\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK2X59XLTLdb",
        "outputId": "fd4cfcf3-af59-47ed-83fb-b6c885c9f8a5"
      },
      "source": [
        "#Applying parts of speech\n",
        "print(lemma.lemmatize('running',pos='v'))\n",
        "print(lemma.lemmatize('runs',pos='v'))\n",
        "print(lemma.lemmatize('ran',pos='v'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run\n",
            "run\n",
            "run\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb4OryPJTLdd",
        "outputId": "696ec082-67b3-4ed1-9f4b-9ebb488922dc"
      },
      "source": [
        "#Both stemming and lemmatization together\n",
        "text=\"Bring King Going Anything Sing Ring Nothing Thing\"\n",
        "\n",
        "#Stemming\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "ps=PorterStemmer()\n",
        "\n",
        "tokens=nltk.word_tokenize(text)\n",
        "for w in tokens:\n",
        "    print(\"Stemming for {} is {}\".format(w,ps.stem(w)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stemming for Bring is bring\n",
            "Stemming for King is king\n",
            "Stemming for Going is go\n",
            "Stemming for Anything is anyth\n",
            "Stemming for Sing is sing\n",
            "Stemming for Ring is ring\n",
            "Stemming for Nothing is noth\n",
            "Stemming for Thing is thing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxpEVcKPTLde",
        "outputId": "7559b606-76d3-40f7-ea37-646ee6725c4f"
      },
      "source": [
        "#Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wl=WordNetLemmatizer()\n",
        "\n",
        "tokens=nltk.word_tokenize(text)\n",
        "for w in tokens:\n",
        "     print(\"Lemmatization for {} is {}\".format(w,wl.lemmatize(w)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lemmatization for Bring is Bring\n",
            "Lemmatization for King is King\n",
            "Lemmatization for Going is Going\n",
            "Lemmatization for Anything is Anything\n",
            "Lemmatization for Sing is Sing\n",
            "Lemmatization for Ring is Ring\n",
            "Lemmatization for Nothing is Nothing\n",
            "Lemmatization for Thing is Thing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1pTDmiRTLdf"
      },
      "source": [
        "# Count Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GUV0i4ETLdf"
      },
      "source": [
        "Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uTbeHgOTLdg"
      },
      "source": [
        "#Without stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2lfXEZgTLdh",
        "outputId": "3028ffac-d8e7-4e1d-fb02-86272781a1cb"
      },
      "source": [
        "#Single document(, separates each document)\n",
        "string=[\"This is an example of bag of words!\"]\n",
        "\n",
        "#Coverting text into tokens\n",
        "vect=CountVectorizer()\n",
        "vect.fit_transform(string)\n",
        "print(\"Bag of Words: \",vect.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bag of Words:  ['an', 'bag', 'example', 'is', 'of', 'this', 'words']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJdvSiFYTLdi"
      },
      "source": [
        "We can see that the data stored is in alphabetical order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaA2EcUBTLdi",
        "outputId": "3aae96f1-4357-4949-a6b3-2e11e7aed589"
      },
      "source": [
        "vect.vocabulary_  #Assigning index to the data which is in alphabetical order"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this': 5, 'is': 3, 'an': 0, 'example': 2, 'of': 4, 'bag': 1, 'words': 6}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18KCcCMOTLdj",
        "outputId": "4de522e5-4c6a-4052-ee34-74dc99dfaf64"
      },
      "source": [
        "#Fit, transform and predict if the word is present or not\n",
        "c_vect=CountVectorizer()\n",
        "c_vect.fit(string)\n",
        "\n",
        "string2=['Lets understand is of words is']\n",
        "c_newvect=c_vect.transform(string2)\n",
        "print(\"Text present at: \",c_newvect.toarray())\n",
        "\n",
        "#Compare with the indexes\n",
        "print(\"Original indexes: \",vect.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text present at:  [[0 0 0 2 1 0 1]]\n",
            "Original indexes:  ['an', 'bag', 'example', 'is', 'of', 'this', 'words']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_gtDKLNTLdk"
      },
      "source": [
        "We are checking both texts and comparing that what words are present and its frequency are the numbers shown"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpJw6mxmTLdk",
        "outputId": "9cead583-3950-427f-b101-9698849bd141"
      },
      "source": [
        "#Bag of words using stopwords\n",
        "sw=stopwords.words('english')\n",
        "string=[\"This is an example of bag of words!\"]\n",
        "vect=CountVectorizer(stop_words=sw)\n",
        "print(vect)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CountVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
            "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
            "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
            "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
            "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
            "                            'itself', ...])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTw8hel5TLdl",
        "outputId": "db55e76b-800a-4b73-88b2-6ce28a758d94"
      },
      "source": [
        "vect.fit_transform(string)\n",
        "print(\"Bag of Words: \",vect.get_feature_names())\n",
        "print(\"Vocabulary:\", vect.vocabulary_)\n",
        "\n",
        "#Printing after removing stop words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bag of Words:  ['bag', 'example', 'words']\n",
            "Vocabulary: {'example': 1, 'bag': 0, 'words': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuGS8HEwTLdm"
      },
      "source": [
        "#Using function\n",
        "def text_matrix(message , countvect):   #countvect is the model and message is text\n",
        "    terms_doc=countvect.fit_transform(message)\n",
        "    return pd.DataFrame(terms_doc.toarray(),columns=countvect.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPWr_AsNTLdn",
        "outputId": "15a70f4b-9b6a-4af6-d2ed-d890ed01c1a0"
      },
      "source": [
        "message=['We are slowly slowly making progress in Natural Language Processing',\n",
        "        'We will get there','But practice is the only mantra for success']\n",
        "c_vect=CountVectorizer()\n",
        "print(\"Below matrix is the Bag of Words approach\")\n",
        "text_matrix(message,c_vect)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Below matrix is the Bag of Words approach\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>are</th>\n",
              "      <th>but</th>\n",
              "      <th>for</th>\n",
              "      <th>get</th>\n",
              "      <th>in</th>\n",
              "      <th>is</th>\n",
              "      <th>language</th>\n",
              "      <th>making</th>\n",
              "      <th>mantra</th>\n",
              "      <th>natural</th>\n",
              "      <th>only</th>\n",
              "      <th>practice</th>\n",
              "      <th>processing</th>\n",
              "      <th>progress</th>\n",
              "      <th>slowly</th>\n",
              "      <th>success</th>\n",
              "      <th>the</th>\n",
              "      <th>there</th>\n",
              "      <th>we</th>\n",
              "      <th>will</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   are  but  for  get  in  is  language  making  mantra  natural  only  \\\n",
              "0    1    0    0    0   1   0         1       1       0        1     0   \n",
              "1    0    0    0    1   0   0         0       0       0        0     0   \n",
              "2    0    1    1    0   0   1         0       0       1        0     1   \n",
              "\n",
              "   practice  processing  progress  slowly  success  the  there  we  will  \n",
              "0         0           1         1       2        0    0      0   1     0  \n",
              "1         0           0         0       0        0    0      1   1     1  \n",
              "2         1           0         0       0        1    1      0   0     0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qgM5fHwTLdn"
      },
      "source": [
        "Here 0 means not present whereas 1 and 2 means the frequency of words present at 0th text,1st and 2nd index texts present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XDOqUy0TLdo"
      },
      "source": [
        "# n-gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFbYVcz8TLdp",
        "outputId": "94a0dd22-9258-4275-fd38-d6ce5c790f44"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "string=[\"This is an example of gram!\"]\n",
        "vect1=CountVectorizer(ngram_range=(1,1))\n",
        "vect1.fit_transform(string)\n",
        "\n",
        "vect2=CountVectorizer(ngram_range=(2,2))\n",
        "vect2.fit_transform(string)\n",
        "\n",
        "vect3=CountVectorizer(ngram_range=(3,3))\n",
        "vect3.fit_transform(string)\n",
        "\n",
        "vect4=CountVectorizer(ngram_range=(4,4))\n",
        "vect4.fit_transform(string)\n",
        "\n",
        "print(\"1-gram: \",vect1.get_feature_names())\n",
        "print(\"2-gram: \",vect2.get_feature_names())\n",
        "print(\"3-gram: \",vect3.get_feature_names())\n",
        "print(\"4-gram: \",vect4.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1-gram:  ['an', 'example', 'gram', 'is', 'of', 'this']\n",
            "2-gram:  ['an example', 'example of', 'is an', 'of gram', 'this is']\n",
            "3-gram:  ['an example of', 'example of gram', 'is an example', 'this is an']\n",
            "4-gram:  ['an example of gram', 'is an example of', 'this is an example']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwu13L2RTLdq"
      },
      "source": [
        "# tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XcXltYzTLdr",
        "outputId": "47512592-bc95-47b4-cada-e3fcb2310159"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "tfid=TfidfVectorizer(smooth_idf=False)  #for not getting log value as 0\n",
        "doc=[\"This is an example.\",\"We will see how it works.\",\"IDF can be confusing\"]\n",
        "doc_vector=tfid.fit_transform(doc)\n",
        "\n",
        "df=pd.DataFrame(doc_vector.todense(),columns=tfid.get_feature_names())\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>an</th>\n",
              "      <th>be</th>\n",
              "      <th>can</th>\n",
              "      <th>confusing</th>\n",
              "      <th>example</th>\n",
              "      <th>how</th>\n",
              "      <th>idf</th>\n",
              "      <th>is</th>\n",
              "      <th>it</th>\n",
              "      <th>see</th>\n",
              "      <th>this</th>\n",
              "      <th>we</th>\n",
              "      <th>will</th>\n",
              "      <th>works</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.408248</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.408248</td>\n",
              "      <td>0.408248</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.408248</td>\n",
              "      <td>0.408248</td>\n",
              "      <td>0.408248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    an   be  can  confusing  example       how  idf   is        it       see  \\\n",
              "0  0.5  0.0  0.0        0.0      0.5  0.000000  0.0  0.5  0.000000  0.000000   \n",
              "1  0.0  0.0  0.0        0.0      0.0  0.408248  0.0  0.0  0.408248  0.408248   \n",
              "2  0.0  0.5  0.5        0.5      0.0  0.000000  0.5  0.0  0.000000  0.000000   \n",
              "\n",
              "   this        we      will     works  \n",
              "0   0.5  0.000000  0.000000  0.000000  \n",
              "1   0.0  0.408248  0.408248  0.408248  \n",
              "2   0.0  0.000000  0.000000  0.000000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smPR7hhkTLds"
      },
      "source": [
        "#Using function\n",
        "def text_matrix(message , countvect):   #countvect is the model and message is text\n",
        "    terms_doc=countvect.fit_transform(message)\n",
        "    return pd.DataFrame(terms_doc.toarray(),columns=countvect.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwpM36_XTLds",
        "outputId": "e72eb14b-775d-4397-c971-595a65f84e2a"
      },
      "source": [
        "feb_message=[\"What is that covid covid\",\"covid is nothing\",\"covid cases are dropping\"]\n",
        "tf=TfidfVectorizer()\n",
        "text_matrix(feb_message,tf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>are</th>\n",
              "      <th>cases</th>\n",
              "      <th>covid</th>\n",
              "      <th>dropping</th>\n",
              "      <th>is</th>\n",
              "      <th>nothing</th>\n",
              "      <th>that</th>\n",
              "      <th>what</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.592567</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.381519</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.501651</td>\n",
              "      <td>0.501651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.425441</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.547832</td>\n",
              "      <td>0.720333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.546454</td>\n",
              "      <td>0.546454</td>\n",
              "      <td>0.322745</td>\n",
              "      <td>0.546454</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        are     cases     covid  dropping        is   nothing      that  \\\n",
              "0  0.000000  0.000000  0.592567  0.000000  0.381519  0.000000  0.501651   \n",
              "1  0.000000  0.000000  0.425441  0.000000  0.547832  0.720333  0.000000   \n",
              "2  0.546454  0.546454  0.322745  0.546454  0.000000  0.000000  0.000000   \n",
              "\n",
              "       what  \n",
              "0  0.501651  \n",
              "1  0.000000  \n",
              "2  0.000000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLFJhEKXTLdt"
      },
      "source": [
        "Weightage is given on the basis on importance of the word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDG1QlBJTLdu",
        "outputId": "aa6d655d-7a42-49c2-9078-2a112bc047c1"
      },
      "source": [
        "jul_message=[\"What is that covid covid\",\"covid is bad\"]\n",
        "text_matrix(jul_message,tf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bad</th>\n",
              "      <th>covid</th>\n",
              "      <th>is</th>\n",
              "      <th>that</th>\n",
              "      <th>what</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.668501</td>\n",
              "      <td>0.334251</td>\n",
              "      <td>0.469778</td>\n",
              "      <td>0.469778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.704909</td>\n",
              "      <td>0.501549</td>\n",
              "      <td>0.501549</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        bad     covid        is      that      what\n",
              "0  0.000000  0.668501  0.334251  0.469778  0.469778\n",
              "1  0.704909  0.501549  0.501549  0.000000  0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUKIoSpBTLdv"
      },
      "source": [
        "# Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRKxFwy-TLdw",
        "outputId": "1f3b8cfb-722f-41e4-dc72-a7a60cbcdd7a"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.0.1-cp38-cp38-win_amd64.whl (23.9 MB)\n",
            "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gensim) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: Cython==0.29.21 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gensim) (0.29.21)\n",
            "Collecting smart-open>=1.8.1\n",
            "  Using cached smart_open-5.0.0-py3-none-any.whl (56 kB)\n",
            "Installing collected packages: smart-open, gensim\n",
            "Successfully installed gensim-4.0.1 smart-open-5.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7db7ajWTLdx",
        "outputId": "ee24c634-8744-4910-aa5a-bbb7247f946b"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from gensim.models import Word2Vec \n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRGsn4xXTLdy"
      },
      "source": [
        "#Taking random text\n",
        "paragraph=\"Shang-Chi and the Legend of the Ten Rings is an upcoming American superhero film based on the Marvel Comics character Shang-Chi. Produced by Marvel Studios and distributed by Walt Disney Studios Motion Pictures, it is intended to be the 25th film in the Marvel Cinematic Universe (MCU). The film is directed by Destin Daniel Cretton from a screenplay he wrote with David Callaham and Andrew Lanham, and stars Simu Liu as Shang-Chi alongside Awkwafina, Tony Leung, Michelle Yeoh, Fala Chen, Meng'er Zhang, Florian Munteanu, and Ronny Chieng. In the film, Shang-Chi is forced to confront his past after he is drawn into the Ten Rings organization.A film based on Shang-Chi entered development in 2001, but work did not begin in earnest until December 2018 when Callaham was hired. Cretton joined in March 2019, with the project fast-tracked as Marvel's first film with an Asian lead. The film's title and primary cast were announced that July, revealing the film's connection to the Mandarin (Leung) and his Ten Rings organization that appears throughout the MCU. Filming began in February 2020 but was put on hold in March due to the COVID-19 pandemic. Production resumed in August before completing in October. Shooting occurred in Sydney and San Francisco.Shang-Chi and the Legend of the Ten Rings is scheduled to be released in the United States on September 3, 2021, as part of Phase Four of the MCU.According to Margaret Loesch, former president and CEO of Marvel Productions, Stan Lee discussed a potential Shang-Chi film or television series with actor Brandon Lee and his mother Linda Lee during the 1980s, with the intention of having Brandon Lee star as the character in such a project.[13] Brandon's father, martial arts legend Bruce Lee, was the visual inspiration for artist Paul Gulacy when drawing Shang-Chi during his tenure on the Master of Kung Fu comic book series in the 1970s.[14] In 2001, Stephen Norrington signed a deal to direct a Shang-Chi film entitled The Hands of Shang-Chi.[15][16] By 2003, the film was in development at DreamWorks Pictures with Yuen Woo-Ping replacing Norrington as director and Bruce C. McKenna hired to write the screenplay.[17] Ang Lee joined the project as a producer in 2004, but the film did not materialize after that point and the rights to the character reverted to Marvel.[16] In September 2005, Marvel chairman and CEO Avi Arad announced Shang-Chi as one of ten properties being developed as films by the newly formed studio Marvel Studios,[18] after the fledgling company received financing to produce the slate of ten films which were to be distributed by Paramount Pictures.[19]According to Chris Fenton, former president of the Chinese-based film production company DMG Entertainment, who was in talks with Marvel Studios to co-produce their films, Marvel offered to create a teaser featuring either Shang-Chi or the Mandarin for the Chinese market that would be featured at the end of The Avengers (2012). DMG balked at the offer, since the Mandarin's negative stereotypical portrayal in the comics could potentially prevent the film from releasing in China and risk shutting down DMG as a company. Ben Kingsley would eventually portray Trevor Slattery, an impostor posing as the Mandarin, in Iron Man 3 (2013), which DMG co-produced.[20]\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vN2emOmTLdz",
        "outputId": "657ec1ee-6ec7-4658-ebfe-98c9234fd5db"
      },
      "source": [
        "string.punctuation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcJZvFLQTLd0",
        "outputId": "dccc52a4-b7d3-4258-e036-3c1889db452b"
      },
      "source": [
        "#Preprocessing the data\n",
        "paragraph=paragraph.translate(str.maketrans('', '', string.punctuation))\n",
        "#Translating the punctuations into space\n",
        "\n",
        "#re.sub(pattern, replace , string)\n",
        "text=re.sub(r'\\[[0-9]*\\]',' ',paragraph)  #Removing numbers\n",
        "text=re.sub(r'\\s+',' ',text)  #Matching all whitespace characters and replacing more spaces with single space\n",
        "text=text.lower()\n",
        "\n",
        "text=re.sub(r'\\d',' ',text)  #Replacing digits with spcaes\n",
        "text=re.sub(r'\\s+',' ',text)\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shangchi and the legend of the ten rings is an upcoming american superhero film based on the marvel comics character shangchi produced by marvel studios and distributed by walt disney studios motion pictures it is intended to be the th film in the marvel cinematic universe mcu the film is directed by destin daniel cretton from a screenplay he wrote with david callaham and andrew lanham and stars simu liu as shangchi alongside awkwafina tony leung michelle yeoh fala chen menger zhang florian munteanu and ronny chieng in the film shangchi is forced to confront his past after he is drawn into the ten rings organizationa film based on shangchi entered development in but work did not begin in earnest until december when callaham was hired cretton joined in march with the project fasttracked as marvels first film with an asian lead the films title and primary cast were announced that july revealing the films connection to the mandarin leung and his ten rings organization that appears throughout the mcu filming began in february but was put on hold in march due to the covid pandemic production resumed in august before completing in october shooting occurred in sydney and san franciscoshangchi and the legend of the ten rings is scheduled to be released in the united states on september as part of phase four of the mcuaccording to margaret loesch former president and ceo of marvel productions stan lee discussed a potential shangchi film or television series with actor brandon lee and his mother linda lee during the s with the intention of having brandon lee star as the character in such a project brandons father martial arts legend bruce lee was the visual inspiration for artist paul gulacy when drawing shangchi during his tenure on the master of kung fu comic book series in the s in stephen norrington signed a deal to direct a shangchi film entitled the hands of shangchi by the film was in development at dreamworks pictures with yuen wooping replacing norrington as director and bruce c mckenna hired to write the screenplay ang lee joined the project as a producer in but the film did not materialize after that point and the rights to the character reverted to marvel in september marvel chairman and ceo avi arad announced shangchi as one of ten properties being developed as films by the newly formed studio marvel studios after the fledgling company received financing to produce the slate of ten films which were to be distributed by paramount pictures according to chris fenton former president of the chinesebased film production company dmg entertainment who was in talks with marvel studios to coproduce their films marvel offered to create a teaser featuring either shangchi or the mandarin for the chinese market that would be featured at the end of the avengers dmg balked at the offer since the mandarins negative stereotypical portrayal in the comics could potentially prevent the film from releasing in china and risk shutting down dmg as a company ben kingsley would eventually portray trevor slattery an impostor posing as the mandarin in iron man which dmg coproduced \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwlrKTZmTLd1"
      },
      "source": [
        "99.9% of punctuations will be removed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBpISGpDTLd1",
        "outputId": "d746a3d2-524b-4a41-8c0c-242e22d07ee6"
      },
      "source": [
        "#Preparing the dataset\n",
        "sent=nltk.sent_tokenize(text)\n",
        "print(sent)\n",
        "\n",
        "#The document will be considered as single line as there are no full stops and other punctuations"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['shangchi and the legend of the ten rings is an upcoming american superhero film based on the marvel comics character shangchi produced by marvel studios and distributed by walt disney studios motion pictures it is intended to be the th film in the marvel cinematic universe mcu the film is directed by destin daniel cretton from a screenplay he wrote with david callaham and andrew lanham and stars simu liu as shangchi alongside awkwafina tony leung michelle yeoh fala chen menger zhang florian munteanu and ronny chieng in the film shangchi is forced to confront his past after he is drawn into the ten rings organizationa film based on shangchi entered development in but work did not begin in earnest until december when callaham was hired cretton joined in march with the project fasttracked as marvels first film with an asian lead the films title and primary cast were announced that july revealing the films connection to the mandarin leung and his ten rings organization that appears throughout the mcu filming began in february but was put on hold in march due to the covid pandemic production resumed in august before completing in october shooting occurred in sydney and san franciscoshangchi and the legend of the ten rings is scheduled to be released in the united states on september as part of phase four of the mcuaccording to margaret loesch former president and ceo of marvel productions stan lee discussed a potential shangchi film or television series with actor brandon lee and his mother linda lee during the s with the intention of having brandon lee star as the character in such a project brandons father martial arts legend bruce lee was the visual inspiration for artist paul gulacy when drawing shangchi during his tenure on the master of kung fu comic book series in the s in stephen norrington signed a deal to direct a shangchi film entitled the hands of shangchi by the film was in development at dreamworks pictures with yuen wooping replacing norrington as director and bruce c mckenna hired to write the screenplay ang lee joined the project as a producer in but the film did not materialize after that point and the rights to the character reverted to marvel in september marvel chairman and ceo avi arad announced shangchi as one of ten properties being developed as films by the newly formed studio marvel studios after the fledgling company received financing to produce the slate of ten films which were to be distributed by paramount pictures according to chris fenton former president of the chinesebased film production company dmg entertainment who was in talks with marvel studios to coproduce their films marvel offered to create a teaser featuring either shangchi or the mandarin for the chinese market that would be featured at the end of the avengers dmg balked at the offer since the mandarins negative stereotypical portrayal in the comics could potentially prevent the film from releasing in china and risk shutting down dmg as a company ben kingsley would eventually portray trevor slattery an impostor posing as the mandarin in iron man which dmg coproduced']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZW1Nk-9TLd2",
        "outputId": "fcfac22d-eb50-4319-c6ae-fc873a0f3c89"
      },
      "source": [
        "#Converting the sentence into word tokenize\n",
        "sent_word=[nltk.word_tokenize(sentence) for sentence in sent]\n",
        "print(sent_word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['shangchi', 'and', 'the', 'legend', 'of', 'the', 'ten', 'rings', 'is', 'an', 'upcoming', 'american', 'superhero', 'film', 'based', 'on', 'the', 'marvel', 'comics', 'character', 'shangchi', 'produced', 'by', 'marvel', 'studios', 'and', 'distributed', 'by', 'walt', 'disney', 'studios', 'motion', 'pictures', 'it', 'is', 'intended', 'to', 'be', 'the', 'th', 'film', 'in', 'the', 'marvel', 'cinematic', 'universe', 'mcu', 'the', 'film', 'is', 'directed', 'by', 'destin', 'daniel', 'cretton', 'from', 'a', 'screenplay', 'he', 'wrote', 'with', 'david', 'callaham', 'and', 'andrew', 'lanham', 'and', 'stars', 'simu', 'liu', 'as', 'shangchi', 'alongside', 'awkwafina', 'tony', 'leung', 'michelle', 'yeoh', 'fala', 'chen', 'menger', 'zhang', 'florian', 'munteanu', 'and', 'ronny', 'chieng', 'in', 'the', 'film', 'shangchi', 'is', 'forced', 'to', 'confront', 'his', 'past', 'after', 'he', 'is', 'drawn', 'into', 'the', 'ten', 'rings', 'organizationa', 'film', 'based', 'on', 'shangchi', 'entered', 'development', 'in', 'but', 'work', 'did', 'not', 'begin', 'in', 'earnest', 'until', 'december', 'when', 'callaham', 'was', 'hired', 'cretton', 'joined', 'in', 'march', 'with', 'the', 'project', 'fasttracked', 'as', 'marvels', 'first', 'film', 'with', 'an', 'asian', 'lead', 'the', 'films', 'title', 'and', 'primary', 'cast', 'were', 'announced', 'that', 'july', 'revealing', 'the', 'films', 'connection', 'to', 'the', 'mandarin', 'leung', 'and', 'his', 'ten', 'rings', 'organization', 'that', 'appears', 'throughout', 'the', 'mcu', 'filming', 'began', 'in', 'february', 'but', 'was', 'put', 'on', 'hold', 'in', 'march', 'due', 'to', 'the', 'covid', 'pandemic', 'production', 'resumed', 'in', 'august', 'before', 'completing', 'in', 'october', 'shooting', 'occurred', 'in', 'sydney', 'and', 'san', 'franciscoshangchi', 'and', 'the', 'legend', 'of', 'the', 'ten', 'rings', 'is', 'scheduled', 'to', 'be', 'released', 'in', 'the', 'united', 'states', 'on', 'september', 'as', 'part', 'of', 'phase', 'four', 'of', 'the', 'mcuaccording', 'to', 'margaret', 'loesch', 'former', 'president', 'and', 'ceo', 'of', 'marvel', 'productions', 'stan', 'lee', 'discussed', 'a', 'potential', 'shangchi', 'film', 'or', 'television', 'series', 'with', 'actor', 'brandon', 'lee', 'and', 'his', 'mother', 'linda', 'lee', 'during', 'the', 's', 'with', 'the', 'intention', 'of', 'having', 'brandon', 'lee', 'star', 'as', 'the', 'character', 'in', 'such', 'a', 'project', 'brandons', 'father', 'martial', 'arts', 'legend', 'bruce', 'lee', 'was', 'the', 'visual', 'inspiration', 'for', 'artist', 'paul', 'gulacy', 'when', 'drawing', 'shangchi', 'during', 'his', 'tenure', 'on', 'the', 'master', 'of', 'kung', 'fu', 'comic', 'book', 'series', 'in', 'the', 's', 'in', 'stephen', 'norrington', 'signed', 'a', 'deal', 'to', 'direct', 'a', 'shangchi', 'film', 'entitled', 'the', 'hands', 'of', 'shangchi', 'by', 'the', 'film', 'was', 'in', 'development', 'at', 'dreamworks', 'pictures', 'with', 'yuen', 'wooping', 'replacing', 'norrington', 'as', 'director', 'and', 'bruce', 'c', 'mckenna', 'hired', 'to', 'write', 'the', 'screenplay', 'ang', 'lee', 'joined', 'the', 'project', 'as', 'a', 'producer', 'in', 'but', 'the', 'film', 'did', 'not', 'materialize', 'after', 'that', 'point', 'and', 'the', 'rights', 'to', 'the', 'character', 'reverted', 'to', 'marvel', 'in', 'september', 'marvel', 'chairman', 'and', 'ceo', 'avi', 'arad', 'announced', 'shangchi', 'as', 'one', 'of', 'ten', 'properties', 'being', 'developed', 'as', 'films', 'by', 'the', 'newly', 'formed', 'studio', 'marvel', 'studios', 'after', 'the', 'fledgling', 'company', 'received', 'financing', 'to', 'produce', 'the', 'slate', 'of', 'ten', 'films', 'which', 'were', 'to', 'be', 'distributed', 'by', 'paramount', 'pictures', 'according', 'to', 'chris', 'fenton', 'former', 'president', 'of', 'the', 'chinesebased', 'film', 'production', 'company', 'dmg', 'entertainment', 'who', 'was', 'in', 'talks', 'with', 'marvel', 'studios', 'to', 'coproduce', 'their', 'films', 'marvel', 'offered', 'to', 'create', 'a', 'teaser', 'featuring', 'either', 'shangchi', 'or', 'the', 'mandarin', 'for', 'the', 'chinese', 'market', 'that', 'would', 'be', 'featured', 'at', 'the', 'end', 'of', 'the', 'avengers', 'dmg', 'balked', 'at', 'the', 'offer', 'since', 'the', 'mandarins', 'negative', 'stereotypical', 'portrayal', 'in', 'the', 'comics', 'could', 'potentially', 'prevent', 'the', 'film', 'from', 'releasing', 'in', 'china', 'and', 'risk', 'shutting', 'down', 'dmg', 'as', 'a', 'company', 'ben', 'kingsley', 'would', 'eventually', 'portray', 'trevor', 'slattery', 'an', 'impostor', 'posing', 'as', 'the', 'mandarin', 'in', 'iron', 'man', 'which', 'dmg', 'coproduced']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmB5rYrbTLd3"
      },
      "source": [
        "punc=string.punctuation\n",
        "for i in range(len(sent_word)):\n",
        "    sent_word[i]=[word for word in sent_word[i]\n",
        "                  if word not in stopwords.words('english') if word not in punc]\n",
        "    \n",
        " #Removing stopwords and punctuations   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2VGwjrLTLd3"
      },
      "source": [
        "#Training the word2vec model\n",
        "model=Word2Vec(sent_word,min_count=1)  #one word at a time\n",
        "words=model.wv.key_to_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u60YQ-e6TLd4",
        "outputId": "b32e6703-5ac6-4028-9f8b-695e5962d253"
      },
      "source": [
        "words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'film': 0,\n",
              " 'shangchi': 1,\n",
              " 'marvel': 2,\n",
              " 'lee': 3,\n",
              " 'ten': 4,\n",
              " 'films': 5,\n",
              " 'rings': 6,\n",
              " 'studios': 7,\n",
              " 'dmg': 8,\n",
              " 'pictures': 9,\n",
              " 'mandarin': 10,\n",
              " 'project': 11,\n",
              " 'company': 12,\n",
              " 'legend': 13,\n",
              " 'character': 14,\n",
              " 'mcu': 15,\n",
              " 'based': 16,\n",
              " 'norrington': 17,\n",
              " 'bruce': 18,\n",
              " 'brandon': 19,\n",
              " 'series': 20,\n",
              " 'ceo': 21,\n",
              " 'president': 22,\n",
              " 'september': 23,\n",
              " 'production': 24,\n",
              " 'announced': 25,\n",
              " 'cretton': 26,\n",
              " 'comics': 27,\n",
              " 'march': 28,\n",
              " 'joined': 29,\n",
              " 'hired': 30,\n",
              " 'distributed': 31,\n",
              " 'leung': 32,\n",
              " 'callaham': 33,\n",
              " 'screenplay': 34,\n",
              " 'development': 35,\n",
              " 'former': 36,\n",
              " 'would': 37,\n",
              " 'talks': 38,\n",
              " 'lead': 39,\n",
              " 'first': 40,\n",
              " 'marvels': 41,\n",
              " 'fasttracked': 42,\n",
              " 'since': 43,\n",
              " 'mandarins': 44,\n",
              " 'negative': 45,\n",
              " 'stereotypical': 46,\n",
              " 'december': 47,\n",
              " 'earnest': 48,\n",
              " 'begin': 49,\n",
              " 'work': 50,\n",
              " 'portrayal': 51,\n",
              " 'entered': 52,\n",
              " 'organizationa': 53,\n",
              " 'drawn': 54,\n",
              " 'past': 55,\n",
              " 'confront': 56,\n",
              " 'forced': 57,\n",
              " 'chieng': 58,\n",
              " 'ronny': 59,\n",
              " 'asian': 60,\n",
              " 'offer': 61,\n",
              " 'florian': 62,\n",
              " 'title': 63,\n",
              " 'end': 64,\n",
              " 'pandemic': 65,\n",
              " 'covid': 66,\n",
              " 'due': 67,\n",
              " 'hold': 68,\n",
              " 'put': 69,\n",
              " 'february': 70,\n",
              " 'began': 71,\n",
              " 'filming': 72,\n",
              " 'throughout': 73,\n",
              " 'appears': 74,\n",
              " 'organization': 75,\n",
              " 'avengers': 76,\n",
              " 'connection': 77,\n",
              " 'revealing': 78,\n",
              " 'july': 79,\n",
              " 'balked': 80,\n",
              " 'cast': 81,\n",
              " 'primary': 82,\n",
              " 'munteanu': 83,\n",
              " 'zhang': 84,\n",
              " 'august': 85,\n",
              " 'kingsley': 86,\n",
              " 'th': 87,\n",
              " 'intended': 88,\n",
              " 'risk': 89,\n",
              " 'motion': 90,\n",
              " 'disney': 91,\n",
              " 'walt': 92,\n",
              " 'shutting': 93,\n",
              " 'ben': 94,\n",
              " 'produced': 95,\n",
              " 'eventually': 96,\n",
              " 'universe': 97,\n",
              " 'portray': 98,\n",
              " 'trevor': 99,\n",
              " 'slattery': 100,\n",
              " 'superhero': 101,\n",
              " 'american': 102,\n",
              " 'upcoming': 103,\n",
              " 'impostor': 104,\n",
              " 'posing': 105,\n",
              " 'iron': 106,\n",
              " 'cinematic': 107,\n",
              " 'china': 108,\n",
              " 'menger': 109,\n",
              " 'simu': 110,\n",
              " 'chen': 111,\n",
              " 'fala': 112,\n",
              " 'yeoh': 113,\n",
              " 'michelle': 114,\n",
              " 'could': 115,\n",
              " 'tony': 116,\n",
              " 'awkwafina': 117,\n",
              " 'alongside': 118,\n",
              " 'liu': 119,\n",
              " 'stars': 120,\n",
              " 'directed': 121,\n",
              " 'lanham': 122,\n",
              " 'andrew': 123,\n",
              " 'potentially': 124,\n",
              " 'david': 125,\n",
              " 'wrote': 126,\n",
              " 'prevent': 127,\n",
              " 'releasing': 128,\n",
              " 'daniel': 129,\n",
              " 'destin': 130,\n",
              " 'resumed': 131,\n",
              " 'completing': 132,\n",
              " 'entertainment': 133,\n",
              " 'wooping': 134,\n",
              " 'point': 135,\n",
              " 'materialize': 136,\n",
              " 'producer': 137,\n",
              " 'ang': 138,\n",
              " 'write': 139,\n",
              " 'mckenna': 140,\n",
              " 'c': 141,\n",
              " 'director': 142,\n",
              " 'replacing': 143,\n",
              " 'yuen': 144,\n",
              " 'fu': 145,\n",
              " 'dreamworks': 146,\n",
              " 'hands': 147,\n",
              " 'entitled': 148,\n",
              " 'direct': 149,\n",
              " 'deal': 150,\n",
              " 'signed': 151,\n",
              " 'create': 152,\n",
              " 'stephen': 153,\n",
              " 'book': 154,\n",
              " 'rights': 155,\n",
              " 'reverted': 156,\n",
              " 'chairman': 157,\n",
              " 'avi': 158,\n",
              " 'coproduce': 159,\n",
              " 'chinesebased': 160,\n",
              " 'fenton': 161,\n",
              " 'chris': 162,\n",
              " 'according': 163,\n",
              " 'paramount': 164,\n",
              " 'slate': 165,\n",
              " 'produce': 166,\n",
              " 'financing': 167,\n",
              " 'received': 168,\n",
              " 'offered': 169,\n",
              " 'fledgling': 170,\n",
              " 'studio': 171,\n",
              " 'formed': 172,\n",
              " 'newly': 173,\n",
              " 'developed': 174,\n",
              " 'properties': 175,\n",
              " 'one': 176,\n",
              " 'arad': 177,\n",
              " 'comic': 178,\n",
              " 'kung': 179,\n",
              " 'october': 180,\n",
              " 'part': 181,\n",
              " 'stan': 182,\n",
              " 'productions': 183,\n",
              " 'market': 184,\n",
              " 'man': 185,\n",
              " 'loesch': 186,\n",
              " 'margaret': 187,\n",
              " 'mcuaccording': 188,\n",
              " 'four': 189,\n",
              " 'phase': 190,\n",
              " 'featured': 191,\n",
              " 'master': 192,\n",
              " 'states': 193,\n",
              " 'united': 194,\n",
              " 'released': 195,\n",
              " 'scheduled': 196,\n",
              " 'franciscoshangchi': 197,\n",
              " 'san': 198,\n",
              " 'sydney': 199,\n",
              " 'occurred': 200,\n",
              " 'shooting': 201,\n",
              " 'chinese': 202,\n",
              " 'discussed': 203,\n",
              " 'potential': 204,\n",
              " 'television': 205,\n",
              " 'tenure': 206,\n",
              " 'drawing': 207,\n",
              " 'gulacy': 208,\n",
              " 'paul': 209,\n",
              " 'artist': 210,\n",
              " 'inspiration': 211,\n",
              " 'visual': 212,\n",
              " 'teaser': 213,\n",
              " 'arts': 214,\n",
              " 'martial': 215,\n",
              " 'father': 216,\n",
              " 'brandons': 217,\n",
              " 'star': 218,\n",
              " 'intention': 219,\n",
              " 'linda': 220,\n",
              " 'mother': 221,\n",
              " 'featuring': 222,\n",
              " 'actor': 223,\n",
              " 'either': 224,\n",
              " 'coproduced': 225}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU-UdnggTLd5"
      },
      "source": [
        "For version 4.0, use key_to_index and for 3.0 use vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALcHaLkfTLd5",
        "outputId": "6042ed41-6b7d-4937-a693-0dd943bb9f0d"
      },
      "source": [
        "vector=model.wv['covid']\n",
        "vector  #Testing word vectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-8.4756194e-03,  6.4536324e-03, -5.8017084e-03, -1.7713847e-03,\n",
              "        9.8988332e-04, -1.9513875e-03, -9.0902587e-03,  4.0142061e-03,\n",
              "       -6.5980027e-03, -8.8169584e-03,  3.9029652e-03, -7.7204825e-04,\n",
              "        6.4899935e-03, -7.0074508e-03,  7.7203056e-04, -7.1260473e-04,\n",
              "        5.7345289e-03, -9.7610289e-03,  5.9352159e-03, -9.0281898e-03,\n",
              "       -7.7734725e-03,  7.5105857e-04, -2.6479808e-03, -3.9153900e-03,\n",
              "       -5.3030523e-03,  4.1159424e-03, -9.3146525e-03,  1.1627774e-03,\n",
              "        4.1487599e-03, -2.9257874e-04, -4.3936851e-04,  7.1762665e-03,\n",
              "       -9.5568430e-03, -9.0553192e-03,  4.4342387e-03,  8.7495754e-03,\n",
              "       -7.8064767e-03, -8.3240308e-03, -6.5635721e-04, -9.0955477e-03,\n",
              "        2.1134836e-03,  9.1984561e-03, -6.1288504e-03, -6.7745326e-03,\n",
              "        8.9484751e-03, -7.5343699e-04, -3.7615739e-03, -2.7026534e-03,\n",
              "       -8.7313261e-03, -4.6034874e-03, -4.1033295e-03,  2.5187019e-04,\n",
              "        7.0043380e-04,  7.6228804e-03,  6.8185925e-03, -1.4755911e-03,\n",
              "       -9.9710850e-03,  2.5815722e-03, -6.1739655e-03,  9.1295233e-03,\n",
              "        3.7881271e-03, -1.0010720e-03,  6.5872623e-03, -3.5159155e-03,\n",
              "       -3.7221448e-03, -2.9404610e-04,  8.4236618e-03,  5.9632598e-03,\n",
              "        1.5410953e-03,  3.8191408e-03, -3.0392834e-03,  9.2701036e-03,\n",
              "       -7.0902850e-03, -3.2488669e-03, -5.2678739e-03,  8.7627815e-03,\n",
              "       -3.1354725e-03, -2.3820794e-03,  7.1006780e-03, -9.6919192e-03,\n",
              "       -2.9129400e-03,  2.9321504e-03, -6.3334559e-03,  8.8010402e-03,\n",
              "        1.1468719e-03, -5.2323388e-03,  4.9373084e-03,  8.0262814e-03,\n",
              "        9.4124331e-04,  6.8137627e-03, -6.7882505e-03, -3.5727546e-03,\n",
              "        9.9393399e-03,  5.6681614e-03,  1.6374432e-03, -3.4706309e-03,\n",
              "        4.4395784e-03,  2.5301077e-03,  8.3622243e-03, -3.0314679e-05],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3j6CommTLef",
        "outputId": "2a8d1508-84fb-4142-f0c5-840e3e175c3b"
      },
      "source": [
        "!pip install numpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}